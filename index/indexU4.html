<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="shortcut icon" href="../Imagenes/ab274f8c-2d5a-4030-a11c-3ab50133c24a.jpg" type="image/x-icon">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Concert+One&family=Jersey+15&family=Oswald:wght@200..700&family=Oxygen:wght@300;400;700&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="../css/styleU1.css">
    <title>Unidad 4</title>
</head>

<body>
    <div class="titleGeneral">
        <h1>
            Unidad 4 Aspectos basicos de la computacion paralela
        </h1>
        <label>
            <br><button class="btn-home4"> <a href="Practicas.html">Practicas</a></button>
        </label>
        <label>
            <br><button class="btn-homeU4"> <a href="../index.html">Pagina Principal</a></button>
        </label>
       
    </div>
    <div class="contenedorMain">
        <main>
            <article>


                <h2 id="4.1 Aspectos básicos de la computacion paralela">4.1 Aspectos basicos de la computacion paralela
                </h2>
                <p>
                    La computación paralela se basa en la idea de dividir un problema en tareas más pequeñas y
                    procesarlas de
                    manera
                    simultánea utilizando múltiples recursos de computación. Esto permite un procesamiento más rápido y
                    eficiente en
                    comparación con los enfoques secuenciales tradicionales. Algunos aspectos fundamentales de la
                    computación
                    paralela incluyen la sincronización de tareas, la comunicación entre procesos y la gestión de
                    recursos.
                </p>
            </article>
            <article>
                <h2 id="4.2 Tipos de computacion paralela">4.2 Tipos de computacion paralela</h2>
                <p>
                    Existen varios tipos de computación paralela que se utilizan en diferentes contextos y escenarios.
                    Algunos
                    de
                    los enfoques más comunes incluyen el procesamiento paralelo a nivel de bit, a nivel de instrucción,
                    a nivel
                    de
                    datos y a nivel de tarea. Estos enfoques se diferencian en cómo se dividen y procesan las tareas y
                    los
                    datos.
                </p>
            </article>
            <article>
                <h2 id="4.2.1 Clasificacion">4.2.1 Clasificacion</h2>
                <p>
                    La clasificación de la computación paralela puede realizarse en función de la forma en que se
                    dividen
                    las tareas y los datos, así como de la forma en que se coordinan y comunican los procesos paralelos.
                    Algunas clasificaciones comunes incluyen la computación paralela a nivel de bit, a nivel de
                    instrucción,
                    a nivel de datos y a nivel de tarea.
                </p>
            </article>
            <article>
                <h2 id="4.2.2 Arquitectura de computadores secuenciales">4.2.2 Arquitectura de computadores secuenciales
                </h2>
                <p>
                    La arquitectura de computadores secuencial se refiere a los sistemas informáticos tradicionales en
                    los
                    que las instrucciones se ejecutan una tras otra en secuencia. Este tipo de arquitectura sigue siendo
                    común en muchas computadoras personales y estaciones de trabajo.
                </p>
            </article>
            <article>
                <h2 id="4.2.3 Organizacion de direcciones de memoria">4.2.3 Organizacion de direcciones de memoria </h2>
                <p>
                    La organización de direcciones de memoria se refiere a cómo se asignan y acceden a las direcciones
                    de
                    memoria en un sistema de computación paralela. Esto incluye consideraciones como la memoria
                    compartida,
                    la memoria distribuida y las técnicas de direccionamiento utilizadas para acceder a los datos en
                    paralelo.
                </p>
            </article>
            <article>
                <h2 id="4.3 Sistema de memoria compartida">4.3 Sistema de memoria compartida</h2>
                <p>
                    Los sistemas de memoria compartida son un enfoque de computación paralela en el que múltiples
                    procesadores acceden a una misma área de memoria compartida. Esto permite a los procesadores
                    compartir
                    datos y comunicarse de manera eficiente. Dentro de los sistemas de memoria compartida, existen dos
                    tipos
                    principales de redes: las redes de medio compartida y las redes conmutadas.
                </p>
            </article>
            <article>
                <h2 id="4.3.1.1 Redes de media compartida">4.3.1.1 Redes de media compartida </h2>
                <p>
                    Las redes de medio compartida son un tipo de arquitectura de memoria compartida en la que los
                    procesadores se conectan físicamente a un bus compartido o a una red de interconexión. Los
                    procesadores
                    pueden leer y escribir en la memoria compartida a través de este medio compartido.
                </p>
            </article>
            <article>
                <h2 id="4.3.1.2 Redes conmutadas">4.3.1.2 Redes conmutadas </h2>
                <p>
                    Las redes conmutadas, por otro lado, utilizan interruptores o conmutadores para establecer
                    conexiones
                    entre los procesadores y la memoria compartida. Estas redes ofrecen una mayor escalabilidad y
                    capacidad
                    de comunicación en comparación con las redes de medio compartida.
                </p>
            </article>
            <article>
                <h2 id="4.4 Sistemas de memoria construida">4.4 Sistemas de memoria construida</h2>
                <p>
                    Los sistemas de memoria construida son una forma de organización de la memoria en la computación
                    paralela en la que cada procesador tiene su propia memoria local. Esto permite una mayor
                    independencia
                    entre los procesadores y reduce la necesidad de acceder a una memoria compartida.
                </p>
            </article>
            <article>
                <h2 id="4.5 Casos de estudio">4.5 Casos de estudio </h2>
                <p>
                    En el campo de la computación paralela, existen numerosos casos de estudio que han demostrado la
                    eficacia y los beneficios de los enfoques paralelos en diferentes dominios. Algunos ejemplos
                    incluyen el
                    uso de computación paralela en simulaciones científicas, análisis de grandes conjuntos de datos,
                    renderizado de gráficos y modelado de sistemas complejos.
                </p>
            </article>

        </main>
    </div>
    <div class="titleTema">
        <h4>Temario</h4>
    </div>
    <aside class="Temario">
        <ul>
            <li><a href="#4.1 Aspectos básicos de la computacion paralela">4.1 Aspectos basicos de la computacion
                    paralela</a> </li>
        </ul>
        <ul>
            <li><a href="#4.2 Tipos de computacion paralela">4.2 Tipos de computacion paralela</a></li>
            <li><a href="#4.2.1 Clasificacion">4.2.1 Clasificacion</a></li>
            <li><a href="#4.2.2 Arquitectura de computadores secuenciales">4.2.2 Arquitectura de computadores
                    secuenciales</a></li>
            <li><a href="#4.2.3 Organizacion de direcciones de memoria">4.2.3 Organizacion de direcciones de memoria</a>
            </li>
        </ul>
        <ul>
            <li><a href="#4.3 Sistema de memoria compartida">4.3 Sistema de memoria compartida</a></li>
            <li><a href="#4.3.1.1 Redes de media compartida">4.3.1.1 Redes de media compartida</a></li>
            <li><a href="#4.3.1.2 Redes conmutadas">4.3.1.2 Redes conmutadas</a></li>
        </ul>
        <ul>
            <li><a href="#4.4 Sistemas de memoria construida">4.4 Sistemas de memoria construida</a></li>
        </ul>
        <ul>
            <li><a href="#4.5 Casos de estudio">4.5 Casos de estudio</a></li>
        </ul>

    </aside>
</body>

</html>